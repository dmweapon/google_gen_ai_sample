{"cells":[{"cell_type":"markdown","metadata":{"id":"oAmrBjzC-weK"},"source":["## Gemini Pro 복수 호출 처리( Async, Parallel )\n","\n","Feedback : shins777@gmail.com. \n","\n","* 이 Colab 은 Generative AI 사용시 복수개의 호출을 위한 Async call 과 Pralllel call 을 구현한 예제입니다. \n","* 코드는 Langchain을 기반으로 처리하며 그에 따른 API는 아래 링크 참고하세요.  \n","    * 자세한 정보는 [README.md](https://github.com/shins777/google_gen_ai_sample/blob/main/notebook/gemini/README.md) 파일 참고하세요."]},{"cell_type":"markdown","metadata":{},"source":["### 라이브러리 설치"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6601,"status":"ok","timestamp":1709839142949,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"},"user_tz":-540},"id":"SWV1C2BLrkGR"},"outputs":[],"source":["%pip install --upgrade --quiet langchain langchain-core langchain-google-vertexai"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from IPython.display import display, Markdown"]},{"cell_type":"markdown","metadata":{"id":"kjgasEpKBgMC"},"source":["### GCP 사용자 인증 / 환경설정\n","\n","GCP 인증방법은 아래와 URL 정보를 참고하여 GCP에 접근 하는 환경을 구성해야 합니다. \n","* https://cloud.google.com/docs/authentication?hl=ko\n","* 자세한 정보는 [README.md](https://github.com/shins777/google_gen_ai_sample/blob/main/notebook/gemini/README.md) 파일 참고하세요."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1709839142949,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"},"user_tz":-540},"id":"XHUPlLRgNBgS"},"outputs":[],"source":["#  아래 코드는 Colab 환경에서만 실행해주세요. 다른 환경에서는 동작하지 않습니다.\n","import sys\n","if \"google.colab\" in sys.modules:\n","    from google.colab import auth\n","    auth.authenticate_user()"]},{"cell_type":"markdown","metadata":{},"source":["### GCP 프로젝트 및 리전 설정\n","본인의 GCP 환경에 맞게 아래 설정을 구성하세요.  \n","* 구글의 최신버전인 gemini pro 사용을 권고드립니다.   \n","* 만일, 기본 버전 text bison 을 사용하려한다면, 참조하는 class 가 다르므로 주의하세요.  \n","* 현재 Gemini는 한국리전(asia-northeast3)을 통해서 접근이 가능합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1709839142949,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"},"user_tz":-540},"id":"bE7BP2izR1tF"},"outputs":[],"source":["model_name=\"gemini-pro\"\n","project=\"ai-hangsik\"\n","location=\"asia-northeast3\""]},{"cell_type":"markdown","metadata":{},"source":["### Responsible AI"]},{"cell_type":"markdown","metadata":{},"source":["구글은 아래와 같이 생성형 AI 처리시 Responsible AI 부분으로 Harmcategory 에 따른 LLM 응답 조절이 가능합니다.  \n","구글은 구글의 모델을 사용하는 사용자가 생성된 컨텐츠때문에 피해가 되지 않도록 하기 위해서 Responsible AI라는 내용으로 구글의 모델을 사용하는 사용자를 보고하고 있습니다.\n","이 부분은 특별한 문제가 없다면 생략해도 됩니다.\n","\n","Responsible AI 설정을 위한 클래스입니다.  \n","*   HarmCategory : https://cloud.google.com/vertex-ai/docs/reference/rest/v1/HarmCategory\n","*   HarmBlockThreshold : https://cloud.google.com/php/docs/reference/cloud-ai-platform/0.31.0/V1.SafetySetting.HarmBlockThreshold"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from langchain_google_vertexai import HarmBlockThreshold, HarmCategory\n","\n","safety_settings = {\n","                    HarmCategory.HARM_CATEGORY_UNSPECIFIED: HarmBlockThreshold.BLOCK_NONE,\n","                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n","                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n","                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n","                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE\n","}"]},{"cell_type":"markdown","metadata":{},"source":["### 모델 초기화 및 기본 실행\n","\n","아래 내용은 Langchain 기반에서의 gemini_pro 를 초기화하는 방법입니다.  \n","업무에 따라서 Langchain 을 사용하셔도 좋고, 만일 latency 가 중요한 업무이면 Google native API를 사용하는것도 Latency를 줄이는 방법이 될수 있습니다.\n","\n","* Langchain VertexAI API : https://api.python.langchain.com/en/stable/llms/langchain_google_vertexai.llms.VertexAI.html#langchain_google_vertexai.llms.VertexAI\n","* API 참고(Python SDK) : https://cloud.google.com/vertex-ai/generative-ai/docs/sdk-for-llm/llm-sdk-overview\n","* API 참고(REST API) : https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/overview?hl=ko\n","\n","Vertex AI에 대한 Langchain 라이브러리를 보시려면 아래 URL을 참고하세요.\n","* https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm\n","\n","참고!! : Langchain 에서 Google AI, GoogleGenerativeAI 또는 langchain-google-genai 로 표시된 API는 Consumer 버전의 API이므로 Enteprise(기업)용 사용자는 반드시 위의 Vertex AI 용으로 사용해주세요."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1709839142950,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"},"user_tz":-540},"id":"TqCRrsX9R1qv"},"outputs":[],"source":["from langchain_google_vertexai.llms import VertexAI\n","\n","gemini_pro = VertexAI( model_name=model_name,\n","                  project=project,\n","                  location=location,\n","                  verbose=True,\n","                  streaming=True,\n","                  safety_settings = safety_settings,\n","                  temperature = 0.2,\n","                  top_p = 1,\n","                  top_k = 10\n","                 )"]},{"cell_type":"markdown","metadata":{"id":"AsnvaclC27Vw"},"source":["### Asynch Call\n","비즈니스 로직상에서 Async call 이 필요한 요건에 활용할 수 있니다. 필요에 따라서 배치형태의 호출에도 사용될 수 있습니다.\n","Async 호출은 Latency를 줄이고, 아키텍처 유연성을 확보하기 위해서 필요한 호출 방식입니다.\n","\n","* Langchain Reference : https://api.python.langchain.com/en/stable/llms/langchain_google_vertexai.llms.VertexAI.html#langchain_google_vertexai.llms.VertexAI.agenerate\n","    * VertexA class 내의 함수중 async 가 표시된 함수들을 참고하세요. (ex> async agenerate )\n"]},{"cell_type":"markdown","metadata":{"id":"fecBtU9MWLvm"},"source":["#### 호출에 필요한 함수 정의"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1709839142950,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"},"user_tz":-540},"id":"lwfpkbtQn5kg"},"outputs":[],"source":["import time\n","import asyncio\n","import threading\n","\n","# 현재 일반 사용자 Colab에서 2 core 밖에 없어서 Aysnc 처리시 아래 코드 필요.\n","# https://stackoverflow.com/questions/55409641/asyncio-run-cannot-be-called-from-a-running-event-loop-when-using-jupyter-no\n","import nest_asyncio\n","nest_asyncio.apply()\n","\n","questions = [\"한국의 수도는 어디인가요 ?\",\"일본의 수도는 어디인가요 ?\",\"미국의 수도는 어디인가요 ?\",\"영국의 수도는 어디인가요 ?\",\"프랑스의 수도는 어디인가요 ?\"]\n","\n","def synch_generate(llm: VertexAI, prompt:str):\n","  \"\"\"\n","  LLM Sync Call를 위한 함수 \n","  llm.generate \n","  \"\"\"\n","  resp = llm.generate(prompts =[prompt])\n","  print(f\"Thread:{threading.get_ident()} : {resp.generations[0][0].text}\")\n","\n","def synch_call(llm: VertexAI):  \n","  result = [synch_generate(llm, questions[i]) for i in range(5)]\n","\n","#------------------------------------------------\n","\n","# Define an async function.\n","async def async_generate(llm:VertexAI, prompt:str):\n","  \"\"\"\n","  LLM Sync Call를 위한 함수 \n","  llm.agenerate \n","  \"\"\"  \n","  resp = await llm.agenerate(prompts =[prompt])\n","  print(f\"Thread:{threading.get_ident()} : {resp.generations[0][0].text}\")\n","\n","async def generate_concurrently(llm:VertexAI):\n","  tasks = [async_generate(llm, questions[i] ) for i in range(5)]\n","  await asyncio.gather(*tasks)"]},{"cell_type":"markdown","metadata":{"id":"IoZDGoVnWOQ0"},"source":["#### Sync, Async 호출 비교\n","아래 코드를 통해서 Sync call을 통한 단계적인 호출보다 aync call를 사용한 경우 같은 thread를 사용하더라도 Latency에서 이점을 얻을 수 있습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11462,"status":"ok","timestamp":1709839154395,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"},"user_tz":-540},"id":"6b8UkGyWWHlq","outputId":"5d534724-044b-447d-e593-bda56526d890"},"outputs":[],"source":["\n","start = time.perf_counter()\n","synch_call(gemini_pro)\n","elapsed = time.perf_counter() - start\n","print(f\"Serially executed in {elapsed:0.2f} seconds.\\n\")\n","\n","start = time.perf_counter()\n","asyncio.run(generate_concurrently(gemini_pro))\n","elapsed = time.perf_counter() - start\n","print(f\"Concurrently executed in {elapsed:0.2f} seconds.\\n\")"]},{"cell_type":"markdown","metadata":{"id":"XI-z_g5G2962"},"source":["### Concurrent call ( Multiprocessing )\n","* Async call과 더불어 동시처리에 자주 활용되는 Paralllel call에 대한 예제입니다.\n","* Parallel call 은 여러개의 호출을 Multi thread 기반에서 호출함으로써 CPU core 수에 따라서 대량 배치처리에 대한 Latency를 줄일수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"12kSbsx4V_kG"},"source":["#### 호출에 사용되는 함수 정의\n","Parallel 처리시 차이점을 확인하는것과 함께, Lanchain을 활용할 때와 Gemini Pro 자체 API를 활용할 때와의 Latency도 비교하기 위해서 두가지 함수를 사용합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1709839154396,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"},"user_tz":-540},"id":"HpUhgT3IYv-r"},"outputs":[],"source":["import vertexai\n","from vertexai.preview.generative_models import GenerativeModel, Part\n","import vertexai.preview.generative_models as generative_models\n","\n","vertexai.init(project=\"ai-hangsik\", location=\"us-central1\")\n","\n","def langchain_call(country:str)->str:\n","  \"\"\"\n","  Langchain 기반의 Gemini Pro call\n","  \"\"\"\n","  capital = gemini_pro.invoke(f\"{country} 의 수도는 ?\")\n","\n","  print(f\"[langchain_call] Thread: {threading.get_ident()} arg:{country}  return : {capital}\")\n","  return capital\n","\n","def native_call(country:str)-> str:\n","  \"\"\"\n","  Gemini Pro 자체 API 기반의call\n","  \"\"\"  \n","  model = GenerativeModel(\"gemini-1.0-pro-001\")\n","  responses = model.generate_content(\n","    [f\"{country} 의 수도는 ?\"],\n","  )\n","  capital = responses.text\n","\n","  print(f\"[native_call] Thread: {threading.get_ident()} arg:{country}  return : {capital}\")\n","  return capital"]},{"cell_type":"markdown","metadata":{"id":"gKwniTpJVkUn"},"source":["#### Sequencial call\n","* 아래 코드는 순차적으로 호출할때의 latency를 측정하기 위한 목적입니다.\n","* Lanchain API, Gemini Pro API 를 사용할때 각각을 측정합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8685,"status":"ok","timestamp":1709839163065,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"},"user_tz":-540},"id":"P_1ZYQ453RSw","outputId":"29de245d-ffc0-46a3-9475-53bd1540474e"},"outputs":[],"source":["import time\n","\n","#----[ Lanchain Call ]-----\n","t1 = time.time()\n","langchain_call(\"대한민국\")\n","langchain_call(\"일본\")\n","langchain_call(\"중국\")\n","t2 = time.time()\n","\n","print(f\"Langchain API call : {t2-t1}\\n\")\n","\n","#----[ Native Call ]-----\n","t1 = time.time()\n","native_call(\"대한민국\")\n","native_call(\"일본\")\n","native_call(\"중국\")\n","t2 = time.time()\n","\n","print(f\"Gemini Pro API call : {t2-t1}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"LgNj1cXZVpSy"},"source":["#### ThreadPoolExecutor - map function\n","* 아래 코드는 threadpool을 활용한 multi 호출을 나타내는 예제입니다.\n","* 처리 결과를 보면 다른 쓰레드에 의해서 각각의 Tasks 가 실행된 것을 알수 있습니다. \n","* map 을 사용하여 처리하였으며, map을 arg 와 return 값을 순서처리를 보장하지는 않습니다. \n","    * https://docs.python.org/3/library/concurrent.futures.html"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7866,"status":"ok","timestamp":1709839170929,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"},"user_tz":-540},"id":"b-7sfSjI2_78","outputId":"4224128e-a161-45e1-d177-85a9cf1fb0c0"},"outputs":[],"source":["import threading\n","from concurrent.futures import ThreadPoolExecutor\n","\n","args_list ={\"대한민국\", \"일본\", \"중국\"}\n","\n","#----[ Lanchain Call ]-----\n","t1 = time.time()\n","\n","with ThreadPoolExecutor(max_workers=10) as executor:\n","    results = executor.map(langchain_call, args_list)\n","\n","print(','.join(results))\n","\n","t2 = time.time()\n","\n","print(f\"Langchain API call : {t2-t1}\\n\")\n","\n","#----[ Native Call ]-----\n","\n","t1 = time.time()\n","\n","with ThreadPoolExecutor(max_workers=10) as executor:\n","    results = executor.map(native_call, args_list)\n","\n","print(','.join(results))\n","\n","t2 = time.time()\n","\n","print(f\"Gemini Pro API call : {t2-t1}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"FLtrWBiBVzHy"},"source":["#### ThreadPoolExecutor - submit function\n","* 아래 코드는 threadpool을 활용한 multi 호출을 나타내는 예제입니다.\n","* 처리 결과를 보면 다른 쓰레드에 의해서 각각의 Tasks 가 실행된 것을 알수 있습니다. \n","* submit 을 사용하여 처리하였으며, submit을 사용하면 arg 에 대한 리턴값에 대한 순서를 보장할수 있습닞다.\n","    * https://docs.python.org/3/library/concurrent.futures.html\n","    * 호출된 순서와 리턴된 값의 순서가 동일함.\n","    * 이 부분은 Concurrent 처리시 참고필요."]},{"cell_type":"markdown","metadata":{},"source":["* 동시처리를 위한 Linux core 수 확인.  \n","* Cloud shell container 기준 : 4 cores. \n","* Colab container 기준 : 2 cores"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!nproc"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6070,"status":"ok","timestamp":1709839176997,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"},"user_tz":-540},"id":"xIwT58N_VX1s","outputId":"94229c80-647c-41d4-a91f-411ef41c7149"},"outputs":[],"source":["import time\n","from concurrent.futures import ThreadPoolExecutor\n","import concurrent.futures\n","\n","args_list ={\"대한민국\", \"일본\", \"중국\"}\n","\n","#----[ Lanchain Call ]-----\n","\n","t1 = time.time()\n","\n","with ThreadPoolExecutor(max_workers=10) as executor:\n","\n","    futures = [executor.submit(langchain_call, arg) for arg in args_list]\n","    results = [future.result() for future in concurrent.futures.as_completed(futures)]\n","\n","print(results)\n","\n","t2 = time.time()\n","\n","print(f\"Langchain API call : {t2-t1}\\n\")\n","\n","#----[ Native Call ]-----\n","\n","t1 = time.time()\n","\n","args_list ={\"대한민국\", \"일본\", \"중국\"}\n","\n","with ThreadPoolExecutor(max_workers=10) as executor:\n","\n","    futures = [executor.submit(native_call, arg) for arg in args_list]\n","    results = [future.result() for future in concurrent.futures.as_completed(futures)]\n","\n","print(results)\n","\n","t2 = time.time()\n","\n","print(f\"Gemini Pro API call : {t2-t1}\\n\")\n","\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1ysth26WNEZ89ceoUWyiBNEhJhAKPzZzD","timestamp":1707791310414},{"file_id":"1SMBsO1Oo1C_iwdVlaPxa-nHarsr1LlXw","timestamp":1707790104224},{"file_id":"12QOQwx4nCfn-TVBJz482kmzE9AO7LOEa","timestamp":1706083956041},{"file_id":"1SEMv5rLuJzrFmgv6ijM-a6frQK6vJh-F","timestamp":1685622927692},{"file_id":"17KvSE1Jozr-l8MPgV0qc96RqjDGib6EG","timestamp":1683740067510},{"file_id":"/v2/external/notebooks/snippets/bigquery.ipynb","timestamp":1674946081821}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"}},"nbformat":4,"nbformat_minor":0}
