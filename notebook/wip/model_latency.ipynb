{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latency Check\n",
    "\n",
    "Feedback : shins777@gmail.com. \n",
    "\n",
    "* 이 Colab은 Google model 들의 latency를 확인하기 위한 테스트 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-cloud-aiplatform "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCP 사용자 인증 / 환경설정\n",
    "\n",
    "GCP 인증방법은 아래와 URL 정보를 참고하여 GCP에 접근 하는 환경을 구성해야 합니다. \n",
    "* https://cloud.google.com/docs/authentication?hl=ko\n",
    "* 자세한 정보는 [README.md](https://github.com/shins777/google_gen_ai_sample/blob/main/notebook/gemini/README.md) 파일 참고하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  아래 코드는 Colab 환경에서만 실행해주세요. 다른 환경에서는 동작하지 않습니다.\n",
    "import sys\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCP 프로젝트 및 리전 설정\n",
    "본인의 GCP 환경에 맞게 아래 설정을 구성하세요.  \n",
    "* 구글의 최신버전인 gemini pro 사용을 권고드립니다.   \n",
    "* 만일, 기본 버전 text bison 을 사용하려한다면, 참조하는 class 가 다르므로 주의하세요.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID='ai-hangsik'\n",
    "LOCATION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "prompt = \"생성형 AI가 사회적으로 문제가 될 수 있는 사항들을 알려주세요. \"\n",
    "\n",
    "text_latency = {}\n",
    "text_response = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PaLM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "\n",
    "palm_model_lists = ['text-bison@001','text-bison@002','text-bison-32k@002','text-unicorn@001']\n",
    "\n",
    "\n",
    "for model_str in palm_model_lists:\n",
    "\n",
    "  t1 = time.time()\n",
    "\n",
    "  parameters = {\n",
    "      \"temperature\":0.2,\n",
    "      \"max_output_tokens\":1024,\n",
    "      \"top_p\":0.8,\n",
    "      \"top_k\":10\n",
    "  }\n",
    "\n",
    "  model = TextGenerationModel.from_pretrained(model_str)\n",
    "  response = model.predict(prompt, **parameters)\n",
    "\n",
    "  t2 = time.time()\n",
    "\n",
    "  text_response[model_str] = response.text\n",
    "  text_latency[model_str] = round((t2-t1),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gemini Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "\n",
    "gemini_model_lists = [\"gemini-1.0-pro-001\",\"gemini-1.0-ultra-001\"]\n",
    "\n",
    "\n",
    "for model_str in gemini_model_lists:\n",
    "\n",
    "\n",
    "  model = GenerativeModel(model_str)\n",
    "\n",
    "  t1 = time.time()\n",
    "\n",
    "  responses = model.generate_content(\n",
    "      prompt,\n",
    "      generation_config={\n",
    "          \"temperature\": 0.2,\n",
    "          \"max_output_tokens\": 1024,\n",
    "          \"top_p\": 0.8,\n",
    "          \"top_k\": 10,\n",
    "      },\n",
    "      stream=False\n",
    "  )\n",
    "\n",
    "  t2 = time.time()\n",
    "\n",
    "  text_response[model_str] = responses.text\n",
    "  text_latency[model_str] = round((t2-t1),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "response_df = pd.DataFrame.from_dict(text_response, orient='index', columns=['Response'])\n",
    "response_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latency_df = pd.DataFrame.from_dict(text_latency, orient='index', columns=['Latency'])\n",
    "latency_df.sort_values(by='Latency', ascending=False, inplace=True)\n",
    "latency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax = latency_df.plot(y='Latency', kind='barh', legend=True, rot=0 )\n",
    "ax.bar_label(ax.containers[0], label_type='edge')\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
