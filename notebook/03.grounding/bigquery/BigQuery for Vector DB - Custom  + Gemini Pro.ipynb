{"cells":[{"cell_type":"markdown","metadata":{},"source":["Copyright 2024 shins777@gmail.com\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\");\n","you may not use this file except in compliance with the License.\n","You may obtain a copy of the License at\n","\n","   https://www.apache.org/licenses/LICENSE-2.0\n","\n","Unless required by applicable law or agreed to in writing, software\n","distributed under the License is distributed on an \"AS IS\" BASIS,\n","WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","See the License for the specific language governing permissions and\n","limitations under the License."]},{"cell_type":"markdown","metadata":{},"source":["## BigQuery Vector DB - Custom embedding\n","\n","Feedback : shins777@gmail.com\n","\n","이 예제는 구글의 BigQuery를 Vector database로 활용하는 예제입니다.  \n","현재 상용화되거나, 오픈소스의 다양한 Vector DB가 있지만, BigQuery의 다양한 기능과, VectorDB 의 특화된 기능이 검색 성능을 높히고 효율적인 개발환경을 구성할 수 있습니다.  \n","여기서 사용하는 임베딩 모델은 사용자가 준비한 embedding 모듈을 사용하도록 구성하였습니다.  \n","\n","이 예제는 Langchain API 기준으로 설명합니다.\n","\n","Reference : \n","* https://cloud.google.com/bigquery/docs/vector-search-intro?hl=ko\n","* https://python.langchain.com/docs/integrations/vectorstores/google_bigquery_vector_search\n","* https://api.python.langchain.com/en/stable/embeddings/langchain_core.embeddings.Embeddings.html#langchain_core.embeddings.Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"muv8WPHyhKiN"},"outputs":[],"source":["%pip install --upgrade --quiet sentence_transformers langchain langchain-google-vertexai google-cloud-aiplatform google-cloud-bigquery"]},{"cell_type":"markdown","metadata":{},"source":["### GCP 사용자 인증 / 환경설정\n","\n","GCP 인증방법은 아래와 URL 정보를 참고하여 GCP에 접근 하는 환경을 구성해야 합니다. \n","* https://cloud.google.com/docs/authentication?hl=ko\n","* 자세한 정보는 [README.md](https://github.com/shins777/google_gen_ai_sample/blob/main/notebook/gemini/README.md) 파일 참고하세요."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#  아래 코드는 Colab 환경에서만 실행해주세요. 다른 환경에서는 동작하지 않습니다.\n","import sys\n","if \"google.colab\" in sys.modules:\n","    from google.colab import auth\n","    auth.authenticate_user()"]},{"cell_type":"markdown","metadata":{},"source":["### GCP 프로젝트 및 리전 설정\n","본인의 GCP 환경에 맞게 아래 설정을 구성하세요.  \n","* 구글의 최신버전인 gemini pro 사용을 권고드립니다.   \n","* 만일, 기본 버전 text bison 을 사용하려한다면, 참조하는 class 가 다르므로 주의하세요.  \n","* 현재 Gemini는 한국리전(asia-northeast3)을 통해서 접근이 가능합니다.\n","* 아래 SEARCH_URL Vertex AI Search를 구성 후 Integration 메뉴항목에서 추출된 값입니다. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kCsSaBi7k78s"},"outputs":[],"source":["PROJECT_ID=\"PROJECT_ID\"\n","REGION=\"asia-northeast3\"\n","MODEL = \"gemini-pro\"\n","\n","#set and show gcp project\n","!gcloud config set project {PROJECT_ID}\n","!gcloud config get-value project"]},{"cell_type":"markdown","metadata":{"id":"_Sj-lBeZuAtF"},"source":["### Custom Embedding 설정\n","\n","Custom embedding 사용시 Langchain 에 있는 함수를 활용려면 Ebeddings interface내에 두개의 함수를 구현해야 합니다.\n","\n","*   Ebeddings interface 구현 : https://api.python.langchain.com/en/stable/embeddings/langchain_core.embeddings.Embeddings.html#langchain_core.embeddings.Embeddings\n","*   아래 두 함수 구현\n","  *   abstract embed_documents(texts: List[str]) → List[List[float]]\n","  *   embed_query(text: str) → List[float]\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DWDM8nilgPF7"},"outputs":[],"source":["from langchain_core.embeddings import Embeddings\n","from typing import List\n","\n","from sentence_transformers import SentenceTransformer\n","\n","class Custom_Embedding(Embeddings):\n","\n","  model = None\n","\n","  def __init__(self, model_name: str):\n","    self.model = SentenceTransformer(model_name)\n","\n","  def embed_documents(self, texts: List[str]) -> List[List[float]]:\n","    embeddings = self.model.encode(texts)\n","    return embeddings.tolist()\n","\n","  def embed_query(self, text: str) -> List[float]:\n","    embeddings = self.model.encode([text])\n","    return embeddings.tolist()[0]\n"]},{"cell_type":"markdown","metadata":{"id":"WeSwax73vo_u"},"source":["### Custom Embeddings\n","\n","Hugging face에서 비교적 검증된 임베딩 모듈을 사용합니다. 사용자가 원하는 임베딩 모듈을 구성해주세요.\n","\n","*   https://huggingface.co/snunlp/KR-SBERT-V40K-klueNLI-augSTS\n","*   https://huggingface.co/sentence-transformers/stsb-xlm-r-multilingual\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"owo6WCt-YLek"},"outputs":[],"source":["# EBEDDING_MODEL = \"sentence-transformers/stsb-xlm-r-multilingual\"\n","EBEDDING_MODEL = \"snunlp/KR-SBERT-V40K-klueNLI-augSTS\"\n","embedding = Custom_Embedding(EBEDDING_MODEL)\n"]},{"cell_type":"markdown","metadata":{},"source":["### BigQuery 구성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LoFW8KvOuy-u"},"outputs":[],"source":["from google.cloud import bigquery\n","\n","DATASET = \"vector_db_custom\"\n","TABLE = \"vector_table_custom\"\n","\n","client = bigquery.Client(project=PROJECT_ID, location=REGION)\n","client.create_dataset(dataset=DATASET, exists_ok=True)"]},{"cell_type":"markdown","metadata":{"id":"L91v8Dluw5Rj"},"source":["### BigQuery Vector Search 구성 \n","*   https://python.langchain.com/docs/integrations/vectorstores/bigquery_vector_search\n","*   https://api.python.langchain.com/en/stable/vectorstores/langchain_community.vectorstores.bigquery_vector_search.BigQueryVectorSearch.html#langchain_community.vectorstores.bigquery_vector_search.BigQueryVectorSearch\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"82LYIoCzuy8G"},"outputs":[],"source":["from langchain.vectorstores.utils import DistanceStrategy\n","from langchain_community.vectorstores import BigQueryVectorSearch\n","\n","table = BigQueryVectorSearch(\n","    project_id=PROJECT_ID,\n","    dataset_name=DATASET,\n","    table_name=TABLE,\n","    location=REGION,\n","    embedding=embedding,\n","\n","    #https://api.python.langchain.com/en/stable/vectorstores/langchain_community.vectorstores.utils.DistanceStrategy.html#langchain_community.vectorstores.utils.DistanceStrategy\n","    distance_strategy=DistanceStrategy.COSINE\n","\n",")"]},{"cell_type":"markdown","metadata":{},"source":["#### 데이터 로드"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q_qjYg-3JQO_"},"outputs":[],"source":["import pandas as pd\n","\n","terms = pd.read_csv('./term1.csv',sep=\"|\", encoding='utf-8-sig')\n","terms"]},{"cell_type":"markdown","metadata":{},"source":["#### 데이터 임베딩 수행"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wmd7PgosMksu"},"outputs":[],"source":["import json\n","\n","all_texts = terms['context'].to_list()\n","#metadatas = [ {'context_title': row['context_title'] } for idx, row in terms.iterrows()]\n","#table.add_texts(all_texts_list, metadatas=metadatas)\n","table.add_texts(all_texts)\n"]},{"cell_type":"markdown","metadata":{"id":"lvByHunswECz"},"source":["### Sentence similarity 데이터 조회\n","\n","\n","*   https://api.python.langchain.com/en/stable/vectorstores/langchain_community.vectorstores.bigquery_vector_search.BigQueryVectorSearch.html#langchain_community.vectorstores.bigquery_vector_search.BigQueryVectorSearch.similarity_search\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vYS-0edguy3Z"},"outputs":[],"source":["import time\n","s = time.time()\n","\n","query = \"질문을 넣어주세요.\"\n","\n","docs = table.similarity_search(query, k=5)\n","\n","for doc in docs:\n","  print(doc.page_content)\n","\n","e = time.time() - s\n","print(e)"]},{"cell_type":"markdown","metadata":{"id":"ahe7Wwt-wRtD"},"source":["### Vector 형태의 쿼리 조회"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LLfIdNoiUZWP"},"outputs":[],"source":["query_vector = embedding.embed_query(query)\n","docs = table.similarity_search_by_vector(query_vector, k=5)\n","for doc in docs:\n","  print(doc.page_content)"]},{"cell_type":"markdown","metadata":{"id":"GQLg5bnjxjCg"},"source":["### 쿼리 유사성 다양성 최적화\n","\n","*   https://api.python.langchain.com/en/stable/vectorstores/langchain_community.vectorstores.bigquery_vector_search.BigQueryVectorSearch.html#langchain_community.vectorstores.bigquery_vector_search.BigQueryVectorSearch.max_marginal_relevance_search"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E1vX3uNXxkFc"},"outputs":[],"source":["docs = table.max_marginal_relevance_search(query= query,\n","                                           k=5,\n","                                           fetch_k = 30,\n","                                           lambda_mult = 0.5,\n","                                           brute_force = True\n","                                           )\n","for doc in docs:\n","  print(doc.page_content)"]},{"cell_type":"markdown","metadata":{"id":"5hIgmJSzxnfV"},"source":["### Similarity with Score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TtHFUru9xquv"},"outputs":[],"source":["tuples = table.similarity_search_with_relevance_scores(query, k=5)\n","\n","context ={}\n","\n","for tp in tuples:\n","    context[tp[1]] = tp[0].page_content\n","    # print(f\"==[{tp[1]}]==\")\n","    # print(tp[0].page_content)\n","\n","context"]},{"cell_type":"markdown","metadata":{"id":"-cW0cCibzE6-"},"source":["### Gemini Pro 실행 - BigQuery as a Grounding Service"]},{"cell_type":"markdown","metadata":{"id":"cjwwcmbczUa6"},"source":["*   VertexAI API : https://api.python.langchain.com/en/stable/llms/langchain_google_vertexai.llms.VertexAI.html#langchain_google_vertexai.llms.VertexAI"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bb6gIIOIzXk2"},"outputs":[],"source":["from langchain_google_vertexai.llms import VertexAI\n","\n","gemini_pro = VertexAI( model_name = MODEL,\n","                  project=PROJECT_ID,\n","                  location=REGION,\n","                  verbose=True,\n","                  streaming=False,\n","                  temperature = 0.2,\n","                  top_p = 1,\n","                  top_k = 40\n","                 )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NAx8LxkbzcT5"},"outputs":[],"source":["from langchain.prompts import PromptTemplate\n","from langchain.chains import LLMChain\n","\n","query = \"질문을 넣어주세요.\"\n","\n","prompt = PromptTemplate.from_template(\"\"\"\n","\n","  당신은 법률을 상담하는 AI 어시스턴트입니다.\n","  아래 Question 에 대해서 반드시 Context에 있는 개별 내용을 기반으로 단계적으로 추론해서 근거를 설명하고 답변해주세요.\n","  Context : {context}\n","  Question : {question}\n","\n","  \"\"\")\n","\n","prompt = prompt.format(context=context,\n","                       question=query)\n","\n","print(f\"Prompt : {prompt}\")\n","print(f\"답변 : {gemini_pro.invoke(prompt)}\")\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1KbKBHV4kjHaHLr4HIfykJqgTKwkqe_8-","timestamp":1708161360816},{"file_id":"1Nh--FXE-Zuf9IKz4p5puctDzIbp54bXS","timestamp":1708145654442}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
