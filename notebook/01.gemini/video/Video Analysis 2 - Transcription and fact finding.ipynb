{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1btZCldJxz-DFxshEjVFBksH3cFZ7ZWzY","timestamp":1717886695950}],"toc_visible":true,"authorship_tag":"ABX9TyPxdD7ZGvPvbbBIZlGxPDTW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Disclaimer & Copyright\n","\n","Copyright 2024 Forusone : shins777@gmail.com\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n","\n","https://www.apache.org/licenses/LICENSE-2.0\n","\n","Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."],"metadata":{"id":"OQJknXG7C9Dq"}},{"cell_type":"markdown","source":["# Gemini - Youtube video analysis\n","* This notebook explains how to use Gemini to understand images in multimodality features of Gemini. This code shows how to use Gemini to analyze a Youtube videos with the feature.\n","* The youtube video that is used in this demo is \"https://www.youtube.com/watch?v=nXVvvRhiGjI\", the usage of the videos is just for the educational purpose.\n","* The video owner is KI Campus, please contact them to get permission if you want to use it.\n","* Don't use this Youtube video for the other purpose.\n","* Refer to the link for more information about the Gemini\n"," * ***https://cloud.google.com/vertex-ai/generative-ai/docs/learn/overview***"],"metadata":{"id":"zB93wBDYDCPI"}},{"cell_type":"markdown","source":["# Configuration\n","## Install python packages\n","* Vertex AI SDK for Python\n","  * https://cloud.google.com/python/docs/reference/aiplatform/latest\n","* Vertex AI initialization : aiplatform.init(..)\n","  * https://cloud.google.com/python/docs/reference/aiplatform/latest#initialization\n","* Install pytube to download the Youtube video\n","  * https://github.com/pytube/pytube\n","  * pip install pytube"],"metadata":{"id":"Vuxu3iRDDtcl"}},{"cell_type":"code","source":["%pip install --upgrade --quiet google-cloud-aiplatform"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wevZD-jnD-ft","executionInfo":{"status":"ok","timestamp":1717905861630,"user_tz":-540,"elapsed":28641,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"f5a4d93b-d6d9-490b-f4b8-51051a6ac68a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/5.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/5.1 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["%pip install -q -U pytube"],"metadata":{"id":"481AaKRW__oz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717905875697,"user_tz":-540,"elapsed":14070,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"4ee782a9-b372-43d0-e0eb-2a4e7eac793c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m41.0/57.6 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["from IPython.display import display, Markdown"],"metadata":{"id":"-nhL4T2sKsdp","executionInfo":{"status":"ok","timestamp":1717905875698,"user_tz":-540,"elapsed":16,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Authentication to access to the GCP & Google drive\n","\n","* Use OAuth to access the GCP environment.\n"," * Refer to the authentication methods in GCP : https://cloud.google.com/docs/authentication?hl=ko"],"metadata":{"id":"tCVttGUsEzgj"}},{"cell_type":"code","source":["#  For only colab to authenticate to get an access to the GCP.\n","import sys\n","\n","if \"google.colab\" in sys.modules:\n","    from google.colab import auth\n","    auth.authenticate_user()"],"metadata":{"id":"AJuo1g4bE3-x","executionInfo":{"status":"ok","timestamp":1717905898686,"user_tz":-540,"elapsed":23003,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["* Mount to the google drive to access the .ipynb files in the repository.\n","\n"],"metadata":{"id":"mQoisCsVE6LM"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZhluEayrE_Io","executionInfo":{"status":"ok","timestamp":1717905922998,"user_tz":-540,"elapsed":24315,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"d9e5c4e4-5daa-4600-f14a-fb33580d55e8"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Download the Youtube video to anlyze.\n","* The youtube video that is used in this demo is \"https://www.youtube.com/watch?v=nXVvvRhiGjI\", the usage of the videos is just for the educational purpose.\n","* The video owner is Google, please contact them to get permission if you want to use it."],"metadata":{"id":"ktnOTtoJD2wx"}},{"cell_type":"code","source":["from pytube import YouTube\n","\n","# Download youtube video to your local drive in colab.\n","YouTube('https://www.youtube.com/watch?v=nXVvvRhiGjI').streams.first().download()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":39},"id":"SYt7U21d_zob","executionInfo":{"status":"ok","timestamp":1717905924809,"user_tz":-540,"elapsed":1814,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"8d0f220c-4799-4887-8619-1130aa5fb873"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/Project Astra Our vision for the future of AI assistants.mp4'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["import moviepy.editor\n","moviepy.editor.ipython_display(\"Project Astra Our vision for the future of AI assistants.mp4\", maxduration = 150)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":172,"output_embedded_package_id":"1Dy8BOuHpxNdF8YvasW9yELVnvZ8QKAEF"},"id":"b-OBKUvnHIJ2","executionInfo":{"status":"ok","timestamp":1717905944034,"user_tz":-540,"elapsed":19228,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"7afb6ec1-fde2-4337-f772-6ef84fbd92f9"},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["# Execute the example\n","## Set the environment on GCP Project\n","* Configure project information\n","  * Model name : LLM model name : https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models\n","  * Project Id : prodect id in GCP\n","  * Region : region name in GCP"],"metadata":{"id":"ey_Bv55hIblt"}},{"cell_type":"code","source":["MODEL_NAME=\"gemini-1.5-flash\"\n","PROJECT_ID=\"ai-hangsik\"\n","REGION=\"asia-northeast3\""],"metadata":{"id":"XJwhurOUHIG-","executionInfo":{"status":"ok","timestamp":1717905944035,"user_tz":-540,"elapsed":6,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### Vertex AI initialization\n","Configure Vertex AI and access to the foundation model."],"metadata":{"id":"Xk3nZQQhIiom"}},{"cell_type":"code","source":["import vertexai\n","from vertexai.preview.generative_models import GenerativeModel, Part\n","import vertexai.preview.generative_models as generative_models\n","\n","# Initalizate the current vertex AI execution environment.\n","vertexai.init(project=PROJECT_ID, location=REGION)\n","\n","# Access to the generative model.\n","model = GenerativeModel(MODEL_NAME)"],"metadata":{"id":"HZ6WeWz4HIEW","executionInfo":{"status":"ok","timestamp":1717905944035,"user_tz":-540,"elapsed":5,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["Encoding function for multimodality"],"metadata":{"id":"2QV1mbbBJRGf"}},{"cell_type":"code","source":["import base64\n","\n","def get_encoded_content(location_type, location, mime_type ):\n","  \"\"\"\n","  Get the encoded content object.\n","\n","  location_type :\n","    The type of the location. ( local or GCS )\n","  location :\n","    The file location of the content.\n","  mime_type :\n","    The mime type of the content.\n","\n","  Returns:\n","    The encoded content object.\n","\n","  \"\"\"\n","\n","  content_obj = None\n","\n","  if location_type == \"local\":\n","    with open(location, 'rb') as f:\n","      raw_obj = base64.b64encode(f.read()).decode('utf-8')\n","      content_obj = Part.from_data(data=base64.b64decode(raw_obj), mime_type=mime_type)\n","\n","  elif location_type == \"GCS\":\n","        content_obj = Part.from_uri(location, mime_type=mime_type)\n","  else:\n","    raise ValueError(\"Invalid location type.\")\n","\n","  return content_obj"],"metadata":{"id":"ySzNqqGmHIBm","executionInfo":{"status":"ok","timestamp":1717905944035,"user_tz":-540,"elapsed":5,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["### Function to get the response"],"metadata":{"id":"r4NbkJ4HJYBB"}},{"cell_type":"code","source":["def generate(content_obj, query:str):\n","    \"\"\"\n","    Generate a response from the model.\n","\n","    content_obj :\n","      encoded object being analyzed in the process\n","    query :\n","      query to be sent to the model\n","\n","    Returns:\n","      The generated response.\n","\n","    \"\"\"\n","\n","    # Set model parameter : https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/send-multimodal-prompts#set_model_parameters\n","    generation_config = {\n","        \"max_output_tokens\": 8192,\n","        \"temperature\": 1,\n","        \"top_p\": 0.95,\n","    }\n","\n","    # Configure satey setting : https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes\n","    # Refer to the link to remove : https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes#how_to_remove_automated_response_blocking_for_select_safety_attributes\n","    safety_settings = {\n","        generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n","        generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n","        generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n","        generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n","    }\n","\n","    responses = model.generate_content(\n","        [content_obj, query],\n","        generation_config=generation_config,\n","        safety_settings=safety_settings,\n","        stream=False,\n","    )\n","\n","    return responses.text"],"metadata":{"id":"sMaCwPVGJXX6","executionInfo":{"status":"ok","timestamp":1717905944035,"user_tz":-540,"elapsed":5,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["## Run example"],"metadata":{"id":"bGdT-2WHJuXf"}},{"cell_type":"code","source":["\n","from time import perf_counter\n","\n","t1_start = perf_counter()\n","\n","# When using local storage for the file location.\n","location_type = \"local\"\n","mime_type = \"video/mp4\"\n","\n","repository_root = \".\"\n","file_path = \"/Project Astra Our vision for the future of AI assistants.mp4\"\n","location = repository_root + file_path\n","\n","content_obj = get_encoded_content(location_type, location, mime_type )\n","\n","prompt = \"\"\"\n","You are a helpful assistant that finds the factual information in the video.\n","Follow the instructions below to generate a response.\n","\n","1. Transcribe this video in English and summarize it in Korean\n","2. What was the object beside an glasses on the desk?\n","3. What was the colour of the speaker?\n","4. What kind of system architecture was drawn on the whiteboard\n","\n","\"\"\"\n","\n","outcome = generate(content_obj, prompt)\n","\n","t1_end  = perf_counter()\n","print(f\"Time : {t1_end - t1_start} seconds\\n\\n\")\n","\n","display(Markdown(outcome))\n","\n","\n","# When using Google Cloud Storage for the location type.\n","\n","# location_type = \"GCS\"\n","# mime_type = \"video/mp4\"\n","# repository_root = \"gs://bucket_name\"\n","# file_path = \"/Project Astra Our vision for the future of AI assistants.mp4\"\n","# location = repository_root + file_path"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"-0Lpj5TaJXVQ","executionInfo":{"status":"ok","timestamp":1717905962492,"user_tz":-540,"elapsed":18461,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"7134eaee-3a0b-4f9f-a1e2-58298185801f"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Time : 29.535318595000035 seconds\n","\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Okay, let’s do some tests. Tell me when you see something that makes sound. I see a speaker which makes sound. What is that part of the speaker called? That is the tweeter. It produces high-frequency sounds. Give me a creative alliteration about these. Creative crayons colour cheerfully. They certainly craft colorful creations. What does that part of the code do? This code defines encryption and decryption functions. It seems to use AES-CBC encryption to encode and decode data based on a key and an initialization vector, IV. That’s right. What neighborhood do you think I’m in? This appears to be the King’s Cross area of London. It is known for its railway station and transportation connections. Do you remember where you saw my glasses? Yes, I do. Your glasses were on the desk near a red apple. Do you remember where you saw my glasses? Yes, I do. Your glasses were on the desk near a red apple. All right, give me a band name for this duo. Golden Stripes. Nice. Thanks Gemini.\n\n\n## Korean Summary\n\n이 영상은 구글 딥마인드의 AI 어시스턴트 미래 비전인 프로젝트 아스트라의 데모 영상입니다.  영상 속 사람은 AI 어시스턴트와 여러 가지 대화를 나눕니다. 스피커의 부품 이름을 묻거나, 물건에 대한 재미있는 말을 만들어달라고 요청합니다. 또한, 코드의 역할을 설명해달라고 하거나, 현재 위치를 추측해달라고 하기도 합니다. 마지막으로,  강아지와 호랑이 인형을 보고 듀오 밴드 이름을 지어달라고 요청합니다. AI 어시스턴트는 이러한 모든 질문에 명확하고 정확하게 답변하며, 사용자와 자연스럽게 대화를 나눕니다. \n\n## Answers to the questions\n\n1. The object beside the glasses was a red apple.\n2. The speaker was white.\n3. The system architecture was client-NLB-server-DB."},"metadata":{}}]}]}